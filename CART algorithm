Steps to Implement CART Algorithm for Decision Tree Learning
1. Import Libraries
First, import the necessary libraries.

python
Copy code
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
2. Load and Explore the Dataset
We'll use the Iris dataset for this demonstration, which is a commonly used dataset for classification. This dataset classifies flowers into three species based on four features.

python
Copy code
from sklearn.datasets import load_iris

# Load Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Target

# Display the first few rows of the data
print(pd.DataFrame(X, columns=iris.feature_names).head())
3. Split Data into Training and Test Sets
Split the dataset into training and testing sets to evaluate the model’s performance.

python
Copy code
# Split data into training and test sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
4. Train a CART Decision Tree Classifier
We'll now create and train a decision tree classifier using CART (default in DecisionTreeClassifier in scikit-learn).

python
Copy code
# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Train the model
clf.fit(X_train, y_train)
5. Visualize the Decision Tree
Visualizing the decision tree helps us understand how the algorithm splits the data.

python
Copy code
# Plot the decision tree
plt.figure(figsize=(12, 8))
plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)
plt.show()
6. Evaluate the Model
Evaluate the classifier on the test set to measure performance.

python
Copy code
# Make predictions on the test data
y_pred = clf.predict(X_test)

# Evaluate accuracy and other metrics
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
print("Classification Report:")
print(classification_report(y_test, target_names=iris.target_names))

# Confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
7. Handle Overfitting
Decision trees are prone to overfitting, especially when the tree grows very deep. Overfitting happens when the model fits too closely to the training data and performs poorly on new, unseen data.

One of the solutions to overfitting in decision trees is pruning, which reduces the size of the tree by removing nodes that provide little information. This can be done using either pre-pruning or post-pruning. Here, we'll focus on post-pruning using max_depth and min_samples_leaf to limit the tree’s growth.

8. Apply Post-Pruning to Reduce Overfitting
You can limit the depth of the tree and the number of samples required to split a node, which helps to prevent overfitting.

python
Copy code
# Initialize a pruned Decision Tree with limited depth
pruned_clf = DecisionTreeClassifier(max_depth=3, min_samples_leaf=5, random_state=42)

# Train the pruned classifier
pruned_clf.fit(X_train, y_train)

# Evaluate the pruned classifier
y_pred_pruned = pruned_clf.predict(X_test)

# Accuracy after pruning
pruned_accuracy = accuracy_score(y_test, y_pred_pruned)
print(f"Accuracy after pruning: {pruned_accuracy:.2f}")
print("Classification Report (Pruned):")
print(classification_report(y_test, y_pred_pruned, target_names=iris.target_names))

# Confusion matrix after pruning
print("Confusion Matrix (Pruned):")
print(confusion_matrix(y_test, y_pred_pruned))
9. Visualize the Pruned Tree
After pruning, it is useful to visualize the pruned decision tree to see how the tree was simplified.

python
Copy code
# Plot the pruned decision tree
plt.figure(figsize=(12, 8))
plot_tree(pruned_clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)
plt.show()
10. Classify a New Sample
Once the model is trained, you can classify new samples. For example, if you have a new sample of a flower with specific features, you can predict its species.

python
Copy code
# Example of a new sample: Sepal length, Sepal width, Petal length, Petal width
new_sample = np.array([[5.1, 3.5, 1.4, 0.2]])

# Predict the class of the new sample
new_prediction = pruned_clf.predict(new_sample)
print(f"Predicted class for the new sample: {iris.target_names[new_prediction][0]}")
Conclusion
Overfitting: Unpruned decision trees tend to overfit the training data by becoming too specific. This results in poor performance on the test data.
Pruning: By limiting the depth of the tree (max_depth) and the number of samples required to make a split (min_samples_leaf), we can reduce overfitting and improve generalization to new, unseen data.
Model Interpretation: Decision trees are easy to interpret because the model can be visualized, and the decision paths can be easily traced back to the input features.
By applying the CART algorithm and pruning techniques, we can effectively build robust models for classification tasks while avoiding overfitting.









